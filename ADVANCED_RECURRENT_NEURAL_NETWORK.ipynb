{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADVANCED_RECURRENT_NEURAL_NETWORK",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOChJSNXtC9g",
        "colab_type": "text"
      },
      "source": [
        "# Advanced RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLIxEDq6VhvZ",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/logo.png\" width=150>\n",
        "\n",
        "In this notebook we're going to cover some advanced topics related to RNNs.\n",
        "\n",
        "1. Conditioned hidden state\n",
        "2. Char-level embeddings\n",
        "3. Encoder and decoder\n",
        "4. Attentional mechanisms\n",
        "5. Implementation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41r7MWJnY0m8",
        "colab_type": "text"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJDhjHCHY0_a",
        "colab_type": "code",
        "outputId": "f37e588d-8efc-4b53-d75a-f4e2a867b62b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Load PyTorch library\n",
        "!pip3 install torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0FbOd6IZmzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from argparse import Namespace\n",
        "import collections\n",
        "import copy\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOsqAo4XZpXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Numpy and PyTorch seeds\n",
        "def set_seeds(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        \n",
        "# Creating directories\n",
        "def create_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHfvEzQ9ZweF",
        "colab_type": "code",
        "outputId": "cc41eeb5-72d1-4b21-fd55-178ce4e86a28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Arguments\n",
        "args = Namespace(\n",
        "    seed=1234,\n",
        "    cuda=True,\n",
        "    batch_size=4,\n",
        "    condition_vocab_size=3, # vocabulary for condition possibilities\n",
        "    embedding_dim=100,\n",
        "    rnn_hidden_dim=100,\n",
        "    hidden_dim=100,\n",
        "    num_layers=1,\n",
        "    bidirectional=False,\n",
        ")\n",
        "\n",
        "# Set seeds\n",
        "set_seeds(seed=args.seed, cuda=args.cuda)\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "print(\"Using CUDA: {}\".format(args.cuda))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CUDA: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoMq0eFRvugb",
        "colab_type": "text"
      },
      "source": [
        "# Conditioned RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUsj7HjBp69f",
        "colab_type": "text"
      },
      "source": [
        "Conditioning an RNN is to add extra information that will be helpful towards a prediction. We can encode (embed it) this information and feed it along with the sequential input into our model. For example, suppose in our document classificaiton example in the previous notebook, we knew the publisher of each news article (NYTimes, ESPN, etc.). We could have encoded that information to help with the prediction. There are several different ways of creating a conditioned RNN.\n",
        "\n",
        "**Note**: If the conditioning information is novel for each input in the sequence, just concatenate it along with each time step's input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc8H9JySmtLa",
        "colab_type": "text"
      },
      "source": [
        "1. Make the initial hidden state the encoded information instead of using the initial zerod hidden state. Make sure that the size of the encoded information is the same as the hidden state for the RNN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKlb9SjfpbED",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/conditioned_rnn1.png\" width=400>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbrlQHx2x8Aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFoiV-fqmvRo",
        "colab_type": "code",
        "outputId": "d90267c2-e2a4-4352-a17f-a4b9252b6725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Condition\n",
        "condition = torch.LongTensor([0, 2, 1, 2]) # batch size of 4 with a vocab size of 3\n",
        "condition_embeddings = nn.Embedding(\n",
        "    embedding_dim=args.embedding_dim, # should be same as RNN hidden dim\n",
        "    num_embeddings=args.condition_vocab_size) # of unique conditions\n",
        "\n",
        "# Initialize hidden state\n",
        "num_directions = 1\n",
        "if args.bidirectional:\n",
        "    num_directions = 2\n",
        "    \n",
        "# If using multiple layers and directions, the hidden state needs to match that size\n",
        "hidden_t = condition_embeddings(condition).unsqueeze(0).repeat(\n",
        "    args.num_layers * num_directions, 1, 1).to(args.device) # initial state to RNN\n",
        "print (hidden_t.size())\n",
        "\n",
        "# Feed into RNN\n",
        "# y_out, _ = self.rnn(x_embedded, hidden_t)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REgyaMDgmtHw",
        "colab_type": "text"
      },
      "source": [
        "2. Concatenate the encoded information with the hidden state at each time step. Do not replace the hidden state because the RNN needs that to learn. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUIg5o-dpiZF",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/conditioned_rnn2.png\" width=400>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ-h28o-pi4X",
        "colab_type": "code",
        "outputId": "83cb6b71-ec26-4cb7-9899-9dd110afd431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Initialize hidden state\n",
        "hidden_t = torch.zeros((args.num_layers * num_directions, args.batch_size, args.rnn_hidden_dim))\n",
        "print (hidden_t.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z6hYSIdqBQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def concat_condition(condition_embeddings, condition, hidden_t, num_layers, num_directions):\n",
        "    condition_t = condition_embeddings(condition).unsqueeze(0).repeat(\n",
        "        num_layers * num_directions, 1, 1)\n",
        "    hidden_t = torch.cat([hidden_t, condition_t], 2)\n",
        "    return hidden_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjyzq_s5pixL",
        "colab_type": "code",
        "outputId": "0c2d7428-ab28-42ce-bca7-4d613835a182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Loop through the inputs time steps\n",
        "hiddens = []\n",
        "seq_size = 1\n",
        "for t in range(seq_size):\n",
        "    hidden_t = concat_condition(condition_embeddings, condition, hidden_t, \n",
        "                                args.num_layers, num_directions).to(args.device)\n",
        "    print (hidden_t.size())\n",
        "    \n",
        "    # Feed into RNN\n",
        "    # hidden_t = rnn_cell(x_in[t], hidden_t)\n",
        "    ..."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-0_81jMXg_J",
        "colab_type": "text"
      },
      "source": [
        "# Char-level embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0yUKKpq3pu_",
        "colab_type": "text"
      },
      "source": [
        "Our conv operations will have inputs that are words in a sentence represented at the character level|  $\\in \\mathbb{R}^{NXSXWXE}$  and outputs are embeddings for each word (based on convlutions applied at the character level.) \n",
        "\n",
        "**Word embeddings**: capture the temporal correlations among\n",
        "adjacent tokens so that similar words have similar representations. Ex. \"New Jersey\" is close to \"NJ\" is close to \"Garden State\", etc.\n",
        "\n",
        "**Char embeddings**: create representations that map words at a character level. Ex. \"toy\" and \"toys\" will be close to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SZgVuwebm_4",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/char_embeddings.png\" width=450>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOdIvz0G3O8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Arguments\n",
        "args = Namespace(\n",
        "    seed=1234,\n",
        "    cuda=False,\n",
        "    shuffle=True,\n",
        "    batch_size=64,\n",
        "    vocab_size=20, # vocabulary\n",
        "    seq_size=10, # max length of each sentence\n",
        "    word_size=15, # max length of each word\n",
        "    embedding_dim=100,\n",
        "    num_filters=100, # filters per size\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raztXIeYXYJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, num_input_channels, \n",
        "                 num_output_channels, padding_idx):\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "        # Char-level embedding\n",
        "        self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
        "                                       num_embeddings=num_embeddings,\n",
        "                                       padding_idx=padding_idx)\n",
        "        \n",
        "        # Conv weights\n",
        "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_output_channels, \n",
        "                                             kernel_size=f) for f in [2,3,4]])\n",
        "\n",
        "    def forward(self, x, channel_first=False, apply_softmax=False):\n",
        "        \n",
        "        # x: (N, seq_len, word_len)\n",
        "        input_shape = x.size()\n",
        "        batch_size, seq_len, word_len = input_shape\n",
        "        x = x.view(-1, word_len) # (N*seq_len, word_len)\n",
        "        \n",
        "        # Embedding\n",
        "        x = self.embeddings(x) # (N*seq_len, word_len, embedding_dim)\n",
        "        \n",
        "        # Rearrange input so num_input_channels is in dim 1 (N, embedding_dim, word_len)\n",
        "        if not channel_first:\n",
        "            x = x.transpose(1, 2)\n",
        "        \n",
        "        # Convolution\n",
        "        z = [F.relu(conv(x)) for conv in self.conv]\n",
        "        \n",
        "        # Pooling\n",
        "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z] \n",
        "        z = [zz.view(batch_size, seq_len, -1) for zz in z] # (N, seq_len, embedding_dim)\n",
        "        \n",
        "        # Concat to get char-level embeddings\n",
        "        z = torch.cat(z, 2) # join conv outputs\n",
        "        \n",
        "        return z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzHVs8Xe0Zph",
        "colab_type": "code",
        "outputId": "d3ee52c4-bb03-40e1-d223-f8b725bf7ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Input\n",
        "input_size = (args.batch_size, args.seq_size, args.word_size)\n",
        "x_in = torch.randint(low=0, high=args.vocab_size, size=input_size).long()\n",
        "print (x_in.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10, 15])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B_Xscby2PMQ",
        "colab_type": "code",
        "outputId": "783fd481-5428-4b54-d8f9-fe8b5213f659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Initial char-level embedding model\n",
        "model = Model(embedding_dim=args.embedding_dim, \n",
        "              num_embeddings=args.vocab_size, \n",
        "              num_input_channels=args.embedding_dim, \n",
        "              num_output_channels=args.num_filters,\n",
        "              padding_idx=0)\n",
        "print (model.named_modules)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.named_modules of Model(\n",
            "  (embeddings): Embedding(20, 100, padding_idx=0)\n",
            "  (conv): ModuleList(\n",
            "    (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
            "  )\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DIgeEZFXYR2",
        "colab_type": "code",
        "outputId": "2741dc0a-5d5f-47e1-9e7b-af4e1fb37735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Forward pass to get char-level embeddings\n",
        "z = model(x_in)\n",
        "print (z.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzTscaE10HFA",
        "colab_type": "text"
      },
      "source": [
        "There are several different ways you can use these char-level embeddings:\n",
        "\n",
        "1. Concat char-level embeddings with word-level embeddings, since we have an embedding for each word (at a char-level) and then feed it into an RNN. \n",
        "2. You can feed the char-level embeddings into an RNN to processes them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyCQ13_ckV_c",
        "colab_type": "text"
      },
      "source": [
        "# Encoder and decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sixbu74kbJk",
        "colab_type": "text"
      },
      "source": [
        "So far we've used RNNs to `encode` a sequential input and generate hidden states. We use these hidden states to `decode` the predictions. So far, the encoder was an RNN and the decoder was just a few fully connected layers followed by a softmax layer (for classification). But the encoder and decoder can assume other architectures as well. For example, the decoder could be an RNN that processes the hidden state outputs from the encoder RNN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfK1mAp1dlpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Arguments\n",
        "args = Namespace(\n",
        "    batch_size=64,\n",
        "    embedding_dim=100,\n",
        "    rnn_hidden_dim=100,\n",
        "    hidden_dim=100,\n",
        "    num_layers=1,\n",
        "    bidirectional=False,\n",
        "    dropout=0.1,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_OJFyY97bF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, rnn_hidden_dim, \n",
        "                 num_layers, bidirectional, padding_idx=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        # Embeddings\n",
        "        self.word_embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
        "                                            num_embeddings=num_embeddings,\n",
        "                                            padding_idx=padding_idx)\n",
        "        \n",
        "        # GRU weights\n",
        "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=rnn_hidden_dim, \n",
        "                          num_layers=num_layers, batch_first=True, \n",
        "                          bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, x_in, x_lengths):\n",
        "        \n",
        "        # Word level embeddings\n",
        "        z_word = self.word_embeddings(x_in)\n",
        "   \n",
        "        # Feed into RNN\n",
        "        out, h_n = self.gru(z)\n",
        "        \n",
        "        # Gather the last relevant hidden state\n",
        "        out = gather_last_relevant_hidden(out, x_lengths)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRXtaGPlpyH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, rnn_hidden_dim, hidden_dim, output_dim, dropout_p):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        # FC weights\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, encoder_output, apply_softmax=False):\n",
        "        \n",
        "        # FC layers\n",
        "        z = self.dropout(encoder_output)\n",
        "        z = self.fc1(z)\n",
        "        z = self.dropout(z)\n",
        "        y_pred = self.fc2(z)\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnKyCPVj-OVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, rnn_hidden_dim, \n",
        "                 hidden_dim, num_layers, bidirectional, output_dim, dropout_p, \n",
        "                 padding_idx=0):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = Encoder(embedding_dim, num_embeddings, rnn_hidden_dim, \n",
        "                               num_layers, bidirectional, padding_idx=0)\n",
        "        self.decoder = Decoder(rnn_hidden_dim, hidden_dim, output_dim, dropout_p)\n",
        "        \n",
        "    def forward(self, x_in, x_lengths, apply_softmax=False):\n",
        "        encoder_outputs = self.encoder(x_in, x_lengths)\n",
        "        y_pred = self.decoder(encoder_outputs, apply_softmax)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfeoErsc-Tum",
        "colab_type": "code",
        "outputId": "d1aaca9f-54b7-49fd-d8c9-768594aceebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "model = Model(embedding_dim=args.embedding_dim, num_embeddings=1000, \n",
        "              rnn_hidden_dim=args.rnn_hidden_dim, hidden_dim=args.hidden_dim, \n",
        "              num_layers=args.num_layers, bidirectional=args.bidirectional, \n",
        "              output_dim=4, dropout_p=args.dropout, padding_idx=0)\n",
        "print (model.named_parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.named_parameters of Model(\n",
            "  (encoder): Encoder(\n",
            "    (word_embeddings): Embedding(1000, 100, padding_idx=0)\n",
            "    (gru): GRU(100, 100, batch_first=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (dropout): Dropout(p=0.1)\n",
            "    (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
            "  )\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAsOI6jEmTd0",
        "colab_type": "text"
      },
      "source": [
        "# Attentional mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJN5ft5Sc_kb",
        "colab_type": "text"
      },
      "source": [
        "When processing an input sequence with an RNN, recall that at each time step we process the input and the hidden state at that time step. For many use cases, it's advantageous to have access to the inputs at all time steps and pay selective attention to the them at each time step. For example, in machine translation, it's advantageous to have access to all the words when translating to another language because translations aren't necessarily word for word. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb6A6WfbXje6",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/attention1.jpg\" width=650>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNkayU0rf-ua",
        "colab_type": "text"
      },
      "source": [
        "Attention can sound a bit confusing so let's see what happens at each time step. At time step j, the model has processed inputs $x_0, x_1, x_2, ..., x_j$ and has generted hidden states $h_0, h_1, h_2, ..., h_j$. The idea is to use all the processed hidden states to make the prediction and not just the most recent one. There are several approaches to how we can do this.\n",
        "\n",
        "With **soft attention**, we learn a vector of floating points (probabilities) to multiply with the hidden states to create the context vector.\n",
        "\n",
        "Ex. [0.1, 0.3, 0.1, 0.4, 0.1]\n",
        "\n",
        "With **hard attention**, we can learn a binary vector to multiply with the hidden states to create the context vector. \n",
        "\n",
        "Ex. [0, 0, 0, 1, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYSIAVQqu3Ab",
        "colab_type": "text"
      },
      "source": [
        "We're going to focus on soft attention because it's more widley used and we can visualize how much of each hidden state helps with the prediction, which is great for interpretability. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ch21nZNvDHO",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/attention2.jpg\" width=650>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_jPXuT8xlqd",
        "colab_type": "text"
      },
      "source": [
        "We're going to implement attention in the document classification task below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0iNnQzdxnGvn"
      },
      "source": [
        "# Document classification with RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n38ZJoVZnGaE"
      },
      "source": [
        "We're going to implement the same document classification task as in the previous notebook but we're going to use an attentional interface for interpretability.\n",
        "\n",
        "**Why not machine translation?** Normally, machine translation is the go-to example for demonstrating attention but it's not really practical. How many situations can you think of that require a seq to generate another sequence? Instead we're going to apply attention with our document classification example to see which input tokens are more influential towards predicting the genre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fu7HgEqbnGFY"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "elL6BxtCmNGf",
        "colab": {}
      },
      "source": [
        "from argparse import Namespace\n",
        "import collections\n",
        "import copy\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCf2fLmPbKKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_seeds(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        \n",
        "# Creating directories\n",
        "def create_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c1e05079-af7e-4384-c9d8-7913317a27dd",
        "id": "TTwkuoZdmMlF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "args = Namespace(\n",
        "    seed=1234,\n",
        "    cuda=True,\n",
        "    shuffle=True,\n",
        "    data_file=\"news.csv\",\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"news\",\n",
        "    train_size=0.7,\n",
        "    val_size=0.15,\n",
        "    test_size=0.15,\n",
        "    pretrained_embeddings=None,\n",
        "    cutoff=25,\n",
        "    num_epochs=5,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=128,\n",
        "    embedding_dim=100,\n",
        "    kernels=[3,5],\n",
        "    num_filters=100,\n",
        "    rnn_hidden_dim=128,\n",
        "    hidden_dim=200,\n",
        "    num_layers=1,\n",
        "    bidirectional=False,\n",
        "    dropout_p=0.25,\n",
        ")\n",
        "\n",
        "# Set seeds\n",
        "set_seeds(seed=args.seed, cuda=args.cuda)\n",
        "\n",
        "# Create save dir\n",
        "create_dirs(args.save_dir)\n",
        "\n",
        "# Expand filepaths\n",
        "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
        "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "print(\"Using CUDA: {}\".format(args.cuda))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CUDA: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xfiWhgX5mMQ5"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "baAsxXNFmMCF",
        "colab": {}
      },
      "source": [
        "import urllib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3tJi_HyOmLw-",
        "colab": {}
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/news.csv\"\n",
        "response = urllib.request.urlopen(url)\n",
        "html = response.read()\n",
        "with open(args.data_file, 'wb') as fp:\n",
        "    fp.write(html)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "96ed9711-411d-48c3-f1ec-ca0c7eb0dde9",
        "id": "wrI_df4bmLjB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv(args.data_file, header=0)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Business</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Business</td>\n",
              "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Business</td>\n",
              "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Business</td>\n",
              "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Business</td>\n",
              "      <td>Oil prices soar to all-time record, posing new...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   category                                              title\n",
              "0  Business  Wall St. Bears Claw Back Into the Black (Reuters)\n",
              "1  Business  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
              "2  Business    Oil and Economy Cloud Stocks' Outlook (Reuters)\n",
              "3  Business  Iraq Halts Oil Exports from Main Southern Pipe...\n",
              "4  Business  Oil prices soar to all-time record, posing new..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "84b95d3f-f97e-46b7-e8db-30034bfb1a34",
        "id": "TreK7nqEmLTN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "by_category = collections.defaultdict(list)\n",
        "for _, row in df.iterrows():\n",
        "    by_category[row.category].append(row.to_dict())\n",
        "for category in by_category:\n",
        "    print (\"{0}: {1}\".format(category, len(by_category[category])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Business: 30000\n",
            "Sci/Tech: 30000\n",
            "Sports: 30000\n",
            "World: 30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "35nb3LxLmLCA",
        "colab": {}
      },
      "source": [
        "final_list = []\n",
        "for _, item_list in sorted(by_category.items()):\n",
        "    if args.shuffle:\n",
        "        np.random.shuffle(item_list)\n",
        "    n = len(item_list)\n",
        "    n_train = int(args.train_size*n)\n",
        "    n_val = int(args.val_size*n)\n",
        "    n_test = int(args.test_size*n)\n",
        "\n",
        "  # Give data point a split attribute\n",
        "    for item in item_list[:n_train]:\n",
        "        item['split'] = 'train'\n",
        "    for item in item_list[n_train:n_train+n_val]:\n",
        "        item['split'] = 'val'\n",
        "    for item in item_list[n_train+n_val:]:\n",
        "        item['split'] = 'test'  \n",
        "\n",
        "    # Add to final list\n",
        "    final_list.extend(item_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d8560064-b2a8-447a-cdc1-d0cf32dfa557",
        "id": "Y48IvuSfmK07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "split_df = pd.DataFrame(final_list)\n",
        "split_df[\"split\"].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train    84000\n",
              "val      18000\n",
              "test     18000\n",
              "Name: split, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RWuNBxAXmKk2",
        "outputId": "1125460c-628b-47af-d0b0-3e37d1ac866b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "def preprocess_text(text):\n",
        "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "    \n",
        "split_df.title = split_df.title.apply(preprocess_text)\n",
        "split_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>split</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Business</td>\n",
              "      <td>train</td>\n",
              "      <td>general electric posts higher rd quarter profit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Business</td>\n",
              "      <td>train</td>\n",
              "      <td>lilly to eliminate up to us jobs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Business</td>\n",
              "      <td>train</td>\n",
              "      <td>s amp p lowers america west outlook to negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Business</td>\n",
              "      <td>train</td>\n",
              "      <td>does rand walk the talk on labor policy ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Business</td>\n",
              "      <td>train</td>\n",
              "      <td>housekeeper advocates for changes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   category  split                                            title\n",
              "0  Business  train  general electric posts higher rd quarter profit\n",
              "1  Business  train                 lilly to eliminate up to us jobs\n",
              "2  Business  train  s amp p lowers america west outlook to negative\n",
              "3  Business  train        does rand walk the talk on labor policy ?\n",
              "4  Business  train                housekeeper advocates for changes"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m-a0OpqhmKJc"
      },
      "source": [
        "## Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RUMQ_MwumJ8F",
        "colab": {}
      },
      "source": [
        "class Vocabulary(object):\n",
        "    def __init__(self, token_to_idx=None):\n",
        "\n",
        "        # Token to index\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self.token_to_idx = token_to_idx\n",
        "\n",
        "        # Index to token\n",
        "        self.idx_to_token = {idx: token \\\n",
        "                             for token, idx in self.token_to_idx.items()}\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'token_to_idx': self.token_to_idx}\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        if token in self.token_to_idx:\n",
        "            index = self.token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self.token_to_idx)\n",
        "            self.token_to_idx[token] = index\n",
        "            self.idx_to_token[index] = token\n",
        "        return index\n",
        "\n",
        "    def add_tokens(self, tokens):\n",
        "        return [self.add_token[token] for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        return self.token_to_idx[token]\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        if index not in self.idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self.idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LtYf3lpExBb",
        "colab_type": "code",
        "outputId": "c0f84daf-f276-458c-f155-95b5b830874b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Vocabulary instance\n",
        "category_vocab = Vocabulary()\n",
        "for index, row in df.iterrows():\n",
        "    category_vocab.add_token(row.category)\n",
        "print (category_vocab) # __str__\n",
        "print (len(category_vocab)) # __len__\n",
        "index = category_vocab.lookup_token(\"Business\")\n",
        "print (index)\n",
        "print (category_vocab.lookup_index(index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Vocabulary(size=4)>\n",
            "4\n",
            "0\n",
            "Business\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0zkF6CsE_yH",
        "colab_type": "text"
      },
      "source": [
        "## Sequence vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtntaISyE_1c",
        "colab_type": "text"
      },
      "source": [
        "Next, we're going to create our Vocabulary classes for the article's title, which is a sequence of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovI8QRefEw_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W3ZouuTEw1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceVocabulary(Vocabulary):\n",
        "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
        "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
        "                 end_seq_token=\"<END>\"):\n",
        "\n",
        "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
        "\n",
        "        self.mask_token = mask_token\n",
        "        self.unk_token = unk_token\n",
        "        self.begin_seq_token = begin_seq_token\n",
        "        self.end_seq_token = end_seq_token\n",
        "\n",
        "        self.mask_index = self.add_token(self.mask_token)\n",
        "        self.unk_index = self.add_token(self.unk_token)\n",
        "        self.begin_seq_index = self.add_token(self.begin_seq_token)\n",
        "        self.end_seq_index = self.add_token(self.end_seq_token)\n",
        "        \n",
        "        # Index to token\n",
        "        self.idx_to_token = {idx: token \\\n",
        "                             for token, idx in self.token_to_idx.items()}\n",
        "\n",
        "    def to_serializable(self):\n",
        "        contents = super(SequenceVocabulary, self).to_serializable()\n",
        "        contents.update({'unk_token': self.unk_token,\n",
        "                         'mask_token': self.mask_token,\n",
        "                         'begin_seq_token': self.begin_seq_token,\n",
        "                         'end_seq_token': self.end_seq_token})\n",
        "        return contents\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        return self.token_to_idx.get(token, self.unk_index)\n",
        "    \n",
        "    def lookup_index(self, index):\n",
        "        if index not in self.idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the SequenceVocabulary\" % index)\n",
        "        return self.idx_to_token[index]\n",
        "    \n",
        "    def __str__(self):\n",
        "        return \"<SequenceVocabulary(size=%d)>\" % len(self.token_to_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_idx)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5UHjpi3El37",
        "colab_type": "code",
        "outputId": "be02da6a-5f27-48a3-f9cb-b0d12d35d09f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Get word counts\n",
        "word_counts = Counter()\n",
        "for title in split_df.title:\n",
        "    for token in title.split(\" \"):\n",
        "        if token not in string.punctuation:\n",
        "            word_counts[token] += 1\n",
        "\n",
        "# Create SequenceVocabulary instance\n",
        "title_word_vocab = SequenceVocabulary()\n",
        "for word, word_count in word_counts.items():\n",
        "    if word_count >= args.cutoff:\n",
        "        title_word_vocab.add_token(word)\n",
        "print (title_word_vocab) # __str__\n",
        "print (len(title_word_vocab)) # __len__\n",
        "index = title_word_vocab.lookup_token(\"general\")\n",
        "print (index)\n",
        "print (title_word_vocab.lookup_index(index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SequenceVocabulary(size=4400)>\n",
            "4400\n",
            "4\n",
            "general\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_wja0EfQNpA",
        "colab_type": "text"
      },
      "source": [
        "We're also going to create an instance fo SequenceVocabulary that processes the input on a character level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SpfS0BXP9pz",
        "colab_type": "code",
        "outputId": "db93c508-2b55-4d0f-c1f8-b7d60668c50d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Create SequenceVocabulary instance\n",
        "title_char_vocab = SequenceVocabulary()\n",
        "for title in split_df.title:\n",
        "    for token in title:\n",
        "        title_char_vocab.add_token(token)\n",
        "print (title_char_vocab) # __str__\n",
        "print (len(title_char_vocab)) # __len__\n",
        "index = title_char_vocab.lookup_token(\"g\")\n",
        "print (index)\n",
        "print (title_char_vocab.lookup_index(index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SequenceVocabulary(size=35)>\n",
            "35\n",
            "4\n",
            "g\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dag6H0SFHAG",
        "colab_type": "text"
      },
      "source": [
        "## Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQIfxcUuKwzz",
        "colab_type": "text"
      },
      "source": [
        "Something new that we introduce in this Vectorizer is calculating the length of our input sequence. We will use this later on to extract the last relevant hidden state for each input sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsNtEnhBEl6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NewsVectorizer(object):\n",
        "    def __init__(self, title_word_vocab, title_char_vocab, category_vocab):\n",
        "        self.title_word_vocab = title_word_vocab\n",
        "        self.title_char_vocab = title_char_vocab\n",
        "        self.category_vocab = category_vocab\n",
        "\n",
        "    def vectorize(self, title):\n",
        "       \n",
        "        # Word-level vectorization\n",
        "        word_indices = [self.title_word_vocab.lookup_token(token) for token in title.split(\" \")]\n",
        "        word_indices = [self.title_word_vocab.begin_seq_index] + word_indices + \\\n",
        "            [self.title_word_vocab.end_seq_index]\n",
        "        title_length = len(word_indices)\n",
        "        word_vector = np.zeros(title_length, dtype=np.int64)\n",
        "        word_vector[:len(word_indices)] = word_indices\n",
        "        \n",
        "        # Char-level vectorization\n",
        "        word_length = max([len(word) for word in title.split(\" \")])\n",
        "        char_vector = np.zeros((len(word_vector), word_length), dtype=np.int64)\n",
        "        char_vector[0, :] = self.title_word_vocab.mask_index # <BEGIN>\n",
        "        char_vector[-1, :] = self.title_word_vocab.mask_index # <END>\n",
        "        for i, word in enumerate(title.split(\" \")):\n",
        "            char_vector[i+1,:len(word)] = [title_char_vocab.lookup_token(char) \\\n",
        "                                           for char in word] # i+1 b/c of <BEGIN> token\n",
        "                \n",
        "        return word_vector, char_vector, len(word_indices)\n",
        "    \n",
        "    def unvectorize_word_vector(self, word_vector):\n",
        "        tokens = [self.title_word_vocab.lookup_index(index) for index in word_vector]\n",
        "        title = \" \".join(token for token in tokens)\n",
        "        return title\n",
        "    \n",
        "    def unvectorize_char_vector(self, char_vector):\n",
        "        title = \"\"\n",
        "        for word_vector in char_vector:\n",
        "            for index in word_vector:\n",
        "                if index == self.title_char_vocab.mask_index:\n",
        "                    break\n",
        "                title += self.title_char_vocab.lookup_index(index)\n",
        "            title += \" \"\n",
        "        return title\n",
        "    \n",
        "    @classmethod\n",
        "    def from_dataframe(cls, df, cutoff):\n",
        "        \n",
        "        # Create class vocab\n",
        "        category_vocab = Vocabulary()        \n",
        "        for category in sorted(set(df.category)):\n",
        "            category_vocab.add_token(category)\n",
        "\n",
        "        # Get word counts\n",
        "        word_counts = Counter()\n",
        "        for title in df.title:\n",
        "            for token in title.split(\" \"):\n",
        "                word_counts[token] += 1\n",
        "        \n",
        "        # Create title vocab (word level)\n",
        "        title_word_vocab = SequenceVocabulary()\n",
        "        for word, word_count in word_counts.items():\n",
        "            if word_count >= cutoff:\n",
        "                title_word_vocab.add_token(word)\n",
        "                \n",
        "        # Create title vocab (char level)\n",
        "        title_char_vocab = SequenceVocabulary()\n",
        "        for title in df.title:\n",
        "            for token in title:\n",
        "                title_char_vocab.add_token(token)\n",
        "        \n",
        "        return cls(title_word_vocab, title_char_vocab, category_vocab)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        title_word_vocab = SequenceVocabulary.from_serializable(contents['title_word_vocab'])\n",
        "        title_char_vocab = SequenceVocabulary.from_serializable(contents['title_char_vocab'])\n",
        "        category_vocab = Vocabulary.from_serializable(contents['category_vocab'])\n",
        "        return cls(title_word_vocab=title_word_vocab, \n",
        "                   title_char_vocab=title_char_vocab, \n",
        "                   category_vocab=category_vocab)\n",
        "    \n",
        "    def to_serializable(self):\n",
        "        return {'title_word_vocab': self.title_word_vocab.to_serializable(),\n",
        "                'title_char_vocab': self.title_char_vocab.to_serializable(),\n",
        "                'category_vocab': self.category_vocab.to_serializable()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtRRXU53El9Y",
        "colab_type": "code",
        "outputId": "3c471ae0-4069-48d2-85be-9ea8c1933a0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# Vectorizer instance\n",
        "vectorizer = NewsVectorizer.from_dataframe(split_df, cutoff=args.cutoff)\n",
        "print (vectorizer.title_word_vocab)\n",
        "print (vectorizer.title_char_vocab)\n",
        "print (vectorizer.category_vocab)\n",
        "word_vector, char_vector, title_length = vectorizer.vectorize(preprocess_text(\n",
        "    \"Roger Federer wins the Wimbledon tennis tournament.\"))\n",
        "print (\"word_vector:\", np.shape(word_vector))\n",
        "print (\"char_vector:\", np.shape(char_vector))\n",
        "print (\"title_length:\", title_length)\n",
        "print (word_vector)\n",
        "print (char_vector)\n",
        "print (vectorizer.unvectorize_word_vector(word_vector))\n",
        "print (vectorizer.unvectorize_char_vector(char_vector))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SequenceVocabulary(size=4404)>\n",
            "<SequenceVocabulary(size=35)>\n",
            "<Vocabulary(size=4)>\n",
            "word_vector: (10,)\n",
            "char_vector: (10, 10)\n",
            "title_length: 10\n",
            "[   2    1 4151 1231   25    1 2392 4076   38    3]\n",
            "[[ 0  0  0  0  0  0  0  0  0  0]\n",
            " [ 7 15  4  5  7  0  0  0  0  0]\n",
            " [21  5 18  5  7  5  7  0  0  0]\n",
            " [26 13  6 16  0  0  0  0  0  0]\n",
            " [12 17  5  0  0  0  0  0  0  0]\n",
            " [26 13 23 25  9  5 18 15  6  0]\n",
            " [12  5  6  6 13 16  0  0  0  0]\n",
            " [12 15 20  7  6  8 23  5  6 12]\n",
            " [30  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0]]\n",
            "<BEGIN> <UNK> federer wins the <UNK> tennis tournament . <END>\n",
            " roger federer wins the wimbledon tennis tournament .  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk_QvpVfFM0S",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU7oDdelFMR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB7FHmiSFMXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, df, vectorizer):\n",
        "        self.df = df\n",
        "        self.vectorizer = vectorizer\n",
        "\n",
        "        # Data splits\n",
        "        self.train_df = self.df[self.df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "        self.val_df = self.df[self.df.split=='val']\n",
        "        self.val_size = len(self.val_df)\n",
        "        self.test_df = self.df[self.df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
        "                            'val': (self.val_df, self.val_size),\n",
        "                            'test': (self.test_df, self.test_size)}\n",
        "        self.set_split('train')\n",
        "\n",
        "        # Class weights (for imbalances)\n",
        "        class_counts = df.category.value_counts().to_dict()\n",
        "        def sort_key(item):\n",
        "            return self.vectorizer.category_vocab.lookup_token(item[0])\n",
        "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "        frequencies = [count for _, count in sorted_counts]\n",
        "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, df, cutoff):\n",
        "        train_df = df[df.split=='train']\n",
        "        return cls(df, NewsVectorizer.from_dataframe(train_df, cutoff))\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, df, vectorizer_filepath):\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(df, vectorizer)\n",
        "\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return NewsVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self.vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        self.target_split = split\n",
        "        self.target_df, self.target_size = self.lookup_dict[split]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Dataset(split={0}, size={1})\".format(\n",
        "            self.target_split, self.target_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.target_df.iloc[index]\n",
        "        title_word_vector, title_char_vector, title_length = \\\n",
        "            self.vectorizer.vectorize(row.title)\n",
        "        category_index = self.vectorizer.category_vocab.lookup_token(row.category)\n",
        "        return {'title_word_vector': title_word_vector, \n",
        "                'title_char_vector': title_char_vector, \n",
        "                'title_length': title_length, \n",
        "                'category': category_index}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        return len(self) // batch_size\n",
        "\n",
        "    def generate_batches(self, batch_size, collate_fn, shuffle=True, \n",
        "                         drop_last=False, device=\"cpu\"):\n",
        "        dataloader = DataLoader(dataset=self, batch_size=batch_size,\n",
        "                                collate_fn=collate_fn, shuffle=shuffle, \n",
        "                                drop_last=drop_last)\n",
        "        for data_dict in dataloader:\n",
        "            out_data_dict = {}\n",
        "            for name, tensor in data_dict.items():\n",
        "                out_data_dict[name] = data_dict[name].to(device)\n",
        "            yield out_data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dpb6ZHJFMeb",
        "colab_type": "code",
        "outputId": "3199ec61-4f15-4ced-f253-257040cc0601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Dataset instance\n",
        "dataset = NewsDataset.load_dataset_and_make_vectorizer(df=split_df,\n",
        "                                                       cutoff=args.cutoff)\n",
        "print (dataset) # __str__\n",
        "input_ = dataset[10] # __getitem__\n",
        "print (input_['title_word_vector'])\n",
        "print (input_['title_char_vector'])\n",
        "print (input_['title_length'])\n",
        "print (input_['category'])\n",
        "print (dataset.vectorizer.unvectorize_word_vector(input_['title_word_vector']))\n",
        "print (dataset.vectorizer.unvectorize_char_vector(input_['title_char_vector']))\n",
        "print (dataset.class_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Dataset(split=train, size=84000)\n",
            "[ 2 51  1 52 53 26 54  3]\n",
            "[[ 0  0  0  0  0  0  0  0  0  0]\n",
            " [18  5  9 12  8  0  0  0  0  0]\n",
            " [18 15 18  4  5 16  0  0  0  0]\n",
            " [25  8  6 27  7 20 14 12 11 22]\n",
            " [26 13 12 17  0  0  0  0  0  0]\n",
            " [ 9  8 25 15  7  0  0  0  0  0]\n",
            " [18  5  8  9  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0]]\n",
            "8\n",
            "0\n",
            "<BEGIN> delta <UNK> bankruptcy with labor deal <END>\n",
            " delta dodges bankruptcy with labor deal  \n",
            "tensor([3.3333e-05, 3.3333e-05, 3.3333e-05, 3.3333e-05])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IUIqtbvFUAG",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJV5WlDiFVVz",
        "colab_type": "text"
      },
      "source": [
        "embed  encoder  attend  predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZCzdZZ9FMhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9wipRZt7feC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NewsEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_word_embeddings, num_char_embeddings,\n",
        "                 kernels, num_input_channels, num_output_channels, \n",
        "                 rnn_hidden_dim, num_layers, bidirectional, \n",
        "                 word_padding_idx=0, char_padding_idx=0):\n",
        "        super(NewsEncoder, self).__init__()\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        \n",
        "        # Embeddings\n",
        "        self.word_embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
        "                                            num_embeddings=num_word_embeddings,\n",
        "                                            padding_idx=word_padding_idx)\n",
        "        self.char_embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
        "                                            num_embeddings=num_char_embeddings,\n",
        "                                            padding_idx=char_padding_idx)\n",
        "        \n",
        "        # Conv weights\n",
        "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, \n",
        "                                             num_output_channels, \n",
        "                                             kernel_size=f) for f in kernels])\n",
        "        \n",
        "        \n",
        "        # GRU weights\n",
        "        self.gru = nn.GRU(input_size=embedding_dim*(len(kernels)+1), \n",
        "                          hidden_size=rnn_hidden_dim, num_layers=num_layers, \n",
        "                          batch_first=True, bidirectional=bidirectional)\n",
        "        \n",
        "    def initialize_hidden_state(self, batch_size, rnn_hidden_dim, device):\n",
        "        \"\"\"Modify this to condition the RNN.\"\"\"\n",
        "        num_directions = 1\n",
        "        if self.bidirectional:\n",
        "            num_directions = 2\n",
        "        hidden_t = torch.zeros(self.num_layers * num_directions, \n",
        "                               batch_size, rnn_hidden_dim).to(device)\n",
        "        \n",
        "    def get_char_level_embeddings(self, x):\n",
        "        # x: (N, seq_len, word_len)\n",
        "        input_shape = x.size()\n",
        "        batch_size, seq_len, word_len = input_shape\n",
        "        x = x.view(-1, word_len) # (N*seq_len, word_len)\n",
        "        \n",
        "        # Embedding\n",
        "        x = self.char_embeddings(x) # (N*seq_len, word_len, embedding_dim)\n",
        "        \n",
        "        # Rearrange input so num_input_channels is in dim 1 (N, embedding_dim, word_len)\n",
        "        x = x.transpose(1, 2)\n",
        "        \n",
        "        # Convolution\n",
        "        z = [F.relu(conv(x)) for conv in self.conv]\n",
        "        \n",
        "        # Pooling\n",
        "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z] \n",
        "        z = [zz.view(batch_size, seq_len, -1) for zz in z] # (N, seq_len, embedding_dim)\n",
        "        \n",
        "        # Concat to get char-level embeddings\n",
        "        z = torch.cat(z, 2) # join conv outputs\n",
        "        \n",
        "        return z\n",
        "        \n",
        "    def forward(self, x_word, x_char, x_lengths, device):\n",
        "        \"\"\"\n",
        "        x_word: word level representation (N, seq_size)\n",
        "        x_char: char level representation (N, seq_size, word_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        # Word level embeddings\n",
        "        z_word = self.word_embeddings(x_word)\n",
        "        \n",
        "        # Char level embeddings\n",
        "        z_char = self.get_char_level_embeddings(x=x_char)\n",
        "        \n",
        "        # Concatenate\n",
        "        z = torch.cat([z_word, z_char], 2)\n",
        "        \n",
        "        # Feed into RNN\n",
        "        initial_h = self.initialize_hidden_state(\n",
        "            batch_size=z.size(0), rnn_hidden_dim=self.gru.hidden_size,\n",
        "            device=device)\n",
        "        out, h_n = self.gru(z, initial_h)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeEcdA287gz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NewsDecoder(nn.Module):\n",
        "    def __init__(self, rnn_hidden_dim, hidden_dim, output_dim, dropout_p):\n",
        "        super(NewsDecoder, self).__init__()\n",
        "        \n",
        "        # Attention FC layer\n",
        "        self.fc_attn = nn.Linear(rnn_hidden_dim, rnn_hidden_dim)\n",
        "        self.v = nn.Parameter(torch.rand(rnn_hidden_dim))\n",
        "        \n",
        "        # FC weights\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, encoder_outputs, apply_softmax=False):\n",
        "        \n",
        "        # Attention\n",
        "        z = torch.tanh(self.fc_attn(encoder_outputs))\n",
        "        z = z.transpose(2,1) # [B*H*T]\n",
        "        v = self.v.repeat(encoder_outputs.size(0),1).unsqueeze(1) #[B*1*H]\n",
        "        z = torch.bmm(v,z).squeeze(1) # [B*T]\n",
        "        attn_scores = F.softmax(z, dim=1)\n",
        "        context = torch.matmul(encoder_outputs.transpose(-2, -1), \n",
        "                               attn_scores.unsqueeze(dim=2)).squeeze()\n",
        "        if len(context.size()) == 1:\n",
        "            context = context.unsqueeze(0)\n",
        "        \n",
        "        # FC layers\n",
        "        z = self.dropout(context)\n",
        "        z = self.fc1(z)\n",
        "        z = self.dropout(z)\n",
        "        y_pred = self.fc2(z)\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return attn_scores, y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVDftS-G7gwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NewsModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_word_embeddings, num_char_embeddings,\n",
        "                 kernels, num_input_channels, num_output_channels, \n",
        "                 rnn_hidden_dim, hidden_dim, output_dim, num_layers, \n",
        "                 bidirectional, dropout_p, word_padding_idx, char_padding_idx):\n",
        "        super(NewsModel, self).__init__()\n",
        "        self.encoder = NewsEncoder(embedding_dim, num_word_embeddings,\n",
        "                                   num_char_embeddings, kernels, \n",
        "                                   num_input_channels, num_output_channels, \n",
        "                                   rnn_hidden_dim, num_layers, bidirectional, \n",
        "                                   word_padding_idx, char_padding_idx)\n",
        "        self.decoder = NewsDecoder(rnn_hidden_dim, hidden_dim, output_dim, \n",
        "                                   dropout_p)\n",
        "        \n",
        "    def forward(self, x_word, x_char, x_lengths, device, apply_softmax=False):\n",
        "        encoder_outputs = self.encoder(x_word, x_char, x_lengths, device)\n",
        "        y_pred = self.decoder(encoder_outputs, apply_softmax)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHPYCPd7Fl3M",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3seBMA7FlcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnRKWLekFlnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, dataset, model, model_state_file, save_dir, device, \n",
        "                 shuffle, num_epochs, batch_size, learning_rate, \n",
        "                 early_stopping_criteria):\n",
        "        self.dataset = dataset\n",
        "        self.class_weights = dataset.class_weights.to(device)\n",
        "        self.device = device\n",
        "        self.model = model.to(device)\n",
        "        self.save_dir = save_dir\n",
        "        self.device = device\n",
        "        self.shuffle = shuffle\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
        "        self.train_state = {\n",
        "            'done_training': False,\n",
        "            'stop_early': False, \n",
        "            'early_stopping_step': 0,\n",
        "            'early_stopping_best_val': 1e8,\n",
        "            'early_stopping_criteria': early_stopping_criteria,\n",
        "            'learning_rate': learning_rate,\n",
        "            'epoch_index': 0,\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_loss': -1,\n",
        "            'test_acc': -1,\n",
        "            'model_filename': model_state_file}\n",
        "    \n",
        "    def update_train_state(self):\n",
        "\n",
        "        # Verbose\n",
        "        print (\"[EPOCH]: {0:02d} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
        "          self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
        "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
        "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
        "\n",
        "        # Save one model at least\n",
        "        if self.train_state['epoch_index'] == 0:\n",
        "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
        "            self.train_state['stop_early'] = False\n",
        "\n",
        "        # Save model if performance improved\n",
        "        elif self.train_state['epoch_index'] >= 1:\n",
        "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
        "\n",
        "            # If loss worsened\n",
        "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
        "                # Update step\n",
        "                self.train_state['early_stopping_step'] += 1\n",
        "\n",
        "            # Loss decreased\n",
        "            else:\n",
        "                # Save the best model\n",
        "                if loss_t < self.train_state['early_stopping_best_val']:\n",
        "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
        "\n",
        "                # Reset early stopping step\n",
        "                self.train_state['early_stopping_step'] = 0\n",
        "\n",
        "            # Stop early ?\n",
        "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
        "              >= self.train_state['early_stopping_criteria']\n",
        "        return self.train_state\n",
        "  \n",
        "    def compute_accuracy(self, y_pred, y_target):\n",
        "        _, y_pred_indices = y_pred.max(dim=1)\n",
        "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "        return n_correct / len(y_pred_indices) * 100\n",
        "    \n",
        "    def pad_word_seq(self, seq, length):\n",
        "        vector = np.zeros(length, dtype=np.int64)\n",
        "        vector[:len(seq)] = seq\n",
        "        vector[len(seq):] = self.dataset.vectorizer.title_word_vocab.mask_index\n",
        "        return vector\n",
        "    \n",
        "    def pad_char_seq(self, seq, seq_length, word_length):\n",
        "        vector = np.zeros((seq_length, word_length), dtype=np.int64)\n",
        "        vector.fill(self.dataset.vectorizer.title_char_vocab.mask_index)\n",
        "        for i in range(len(seq)):\n",
        "            char_padding = np.zeros(word_length-len(seq[i]), dtype=np.int64)\n",
        "            vector[i] = np.concatenate((seq[i], char_padding), axis=None)\n",
        "        return vector\n",
        "        \n",
        "    def collate_fn(self, batch):\n",
        "        \n",
        "        # Make a deep copy\n",
        "        batch_copy = copy.deepcopy(batch)\n",
        "        processed_batch = {\"title_word_vector\": [], \"title_char_vector\": [], \n",
        "                           \"title_length\": [], \"category\": []}\n",
        "             \n",
        "        # Max lengths\n",
        "        get_seq_length = lambda sample: len(sample[\"title_word_vector\"])\n",
        "        get_word_length = lambda sample: len(sample[\"title_char_vector\"][0])\n",
        "        max_seq_length = max(map(get_seq_length, batch))\n",
        "        max_word_length = max(map(get_word_length, batch))\n",
        "\n",
        "\n",
        "        # Pad\n",
        "        for i, sample in enumerate(batch_copy):\n",
        "            padded_word_seq = self.pad_word_seq(\n",
        "                sample[\"title_word_vector\"], max_seq_length)\n",
        "            padded_char_seq = self.pad_char_seq(\n",
        "                sample[\"title_char_vector\"], max_seq_length, max_word_length)\n",
        "            processed_batch[\"title_word_vector\"].append(padded_word_seq)\n",
        "            processed_batch[\"title_char_vector\"].append(padded_char_seq)\n",
        "            processed_batch[\"title_length\"].append(sample[\"title_length\"])\n",
        "            processed_batch[\"category\"].append(sample[\"category\"])\n",
        "            \n",
        "        # Convert to appropriate tensor types\n",
        "        processed_batch[\"title_word_vector\"] = torch.LongTensor(\n",
        "            processed_batch[\"title_word_vector\"])\n",
        "        processed_batch[\"title_char_vector\"] = torch.LongTensor(\n",
        "            processed_batch[\"title_char_vector\"])\n",
        "        processed_batch[\"title_length\"] = torch.LongTensor(\n",
        "            processed_batch[\"title_length\"])\n",
        "        processed_batch[\"category\"] = torch.LongTensor(\n",
        "            processed_batch[\"category\"])\n",
        "        \n",
        "        return processed_batch  \n",
        "  \n",
        "    def run_train_loop(self):\n",
        "        for epoch_index in range(self.num_epochs):\n",
        "            self.train_state['epoch_index'] = epoch_index\n",
        "      \n",
        "            # Iterate over train dataset\n",
        "\n",
        "            # initialize batch generator, set loss and acc to 0, set train mode on\n",
        "            self.dataset.set_split('train')\n",
        "            batch_generator = self.dataset.generate_batches(\n",
        "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
        "                shuffle=self.shuffle, device=self.device)\n",
        "            running_loss = 0.0\n",
        "            running_acc = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            for batch_index, batch_dict in enumerate(batch_generator):\n",
        "                # zero the gradients\n",
        "                self.optimizer.zero_grad()\n",
        "                \n",
        "                # compute the output\n",
        "                _, y_pred = self.model(x_word=batch_dict['title_word_vector'],\n",
        "                                       x_char=batch_dict['title_char_vector'],\n",
        "                                       x_lengths=batch_dict['title_length'],\n",
        "                                       device=self.device)\n",
        "                \n",
        "                # compute the loss\n",
        "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
        "                loss_t = loss.item()\n",
        "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                # compute gradients using loss\n",
        "                loss.backward()\n",
        "\n",
        "                # use optimizer to take a gradient step\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                # compute the accuracy\n",
        "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
        "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            self.train_state['train_loss'].append(running_loss)\n",
        "            self.train_state['train_acc'].append(running_acc)\n",
        "\n",
        "            # Iterate over val dataset\n",
        "\n",
        "            # initialize batch generator, set loss and acc to 0, set eval mode on\n",
        "            self.dataset.set_split('val')\n",
        "            batch_generator = self.dataset.generate_batches(\n",
        "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
        "                shuffle=self.shuffle, device=self.device)\n",
        "            running_loss = 0.\n",
        "            running_acc = 0.\n",
        "            self.model.eval()\n",
        "\n",
        "            for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "                # compute the output\n",
        "                _, y_pred = self.model(x_word=batch_dict['title_word_vector'],\n",
        "                                       x_char=batch_dict['title_char_vector'],\n",
        "                                       x_lengths=batch_dict['title_length'],\n",
        "                                       device=self.device)\n",
        "\n",
        "                # compute the loss\n",
        "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
        "                loss_t = loss.to(\"cpu\").item()\n",
        "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                # compute the accuracy\n",
        "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
        "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            self.train_state['val_loss'].append(running_loss)\n",
        "            self.train_state['val_acc'].append(running_acc)\n",
        "\n",
        "            self.train_state = self.update_train_state()\n",
        "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
        "            if self.train_state['stop_early']:\n",
        "                break\n",
        "          \n",
        "    def run_test_loop(self):\n",
        "        # initialize batch generator, set loss and acc to 0, set eval mode on\n",
        "        self.dataset.set_split('test')\n",
        "        batch_generator = self.dataset.generate_batches(\n",
        "            batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
        "            shuffle=self.shuffle, device=self.device)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        self.model.eval()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # compute the output\n",
        "            _, y_pred = self.model(x_word=batch_dict['title_word_vector'],\n",
        "                                   x_char=batch_dict['title_char_vector'],\n",
        "                                   x_lengths=batch_dict['title_length'],\n",
        "                                   device=self.device)\n",
        "\n",
        "            # compute the loss\n",
        "            loss = self.loss_func(y_pred, batch_dict['category'])\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # compute the accuracy\n",
        "            acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "        self.train_state['test_loss'] = running_loss\n",
        "        self.train_state['test_acc'] = running_acc\n",
        "    \n",
        "    def plot_performance(self):\n",
        "        # Figure size\n",
        "        plt.figure(figsize=(15,5))\n",
        "\n",
        "        # Plot Loss\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"Loss\")\n",
        "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
        "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "        # Plot Accuracy\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title(\"Accuracy\")\n",
        "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
        "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        # Save figure\n",
        "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
        "\n",
        "        # Show plots\n",
        "        plt.show()\n",
        "    \n",
        "    def save_train_state(self):\n",
        "        self.train_state[\"done_training\"] = True\n",
        "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
        "            json.dump(self.train_state, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICkiOaGtFlk-",
        "colab_type": "code",
        "outputId": "56dde854-2ced-41a7-a5dc-48a2e4eb1fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# Initialization\n",
        "dataset = NewsDataset.load_dataset_and_make_vectorizer(df=split_df,\n",
        "                                                       cutoff=args.cutoff)\n",
        "dataset.save_vectorizer(args.vectorizer_file)\n",
        "vectorizer = dataset.vectorizer\n",
        "model = NewsModel(embedding_dim=args.embedding_dim, \n",
        "                  num_word_embeddings=len(vectorizer.title_word_vocab), \n",
        "                  num_char_embeddings=len(vectorizer.title_char_vocab),\n",
        "                  kernels=args.kernels,\n",
        "                  num_input_channels=args.embedding_dim,\n",
        "                  num_output_channels=args.num_filters,\n",
        "                  rnn_hidden_dim=args.rnn_hidden_dim,\n",
        "                  hidden_dim=args.hidden_dim,\n",
        "                  output_dim=len(vectorizer.category_vocab),\n",
        "                  num_layers=args.num_layers,\n",
        "                  bidirectional=args.bidirectional,\n",
        "                  dropout_p=args.dropout_p, \n",
        "                  word_padding_idx=vectorizer.title_word_vocab.mask_index,\n",
        "                  char_padding_idx=vectorizer.title_char_vocab.mask_index)\n",
        "print (model.named_modules)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.named_modules of NewsModel(\n",
            "  (encoder): NewsEncoder(\n",
            "    (word_embeddings): Embedding(3406, 100, padding_idx=0)\n",
            "    (char_embeddings): Embedding(35, 100, padding_idx=0)\n",
            "    (conv): ModuleList(\n",
            "      (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "      (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
            "    )\n",
            "    (gru): GRU(300, 128, batch_first=True)\n",
            "  )\n",
            "  (decoder): NewsDecoder(\n",
            "    (fc_attn): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (dropout): Dropout(p=0.25)\n",
            "    (fc1): Linear(in_features=128, out_features=200, bias=True)\n",
            "    (fc2): Linear(in_features=200, out_features=4, bias=True)\n",
            "  )\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuaRZ4DiFlh1",
        "colab_type": "code",
        "outputId": "82a8b0b8-730d-4488-8a6b-4f4db4fbf43b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Train\n",
        "trainer = Trainer(dataset=dataset, model=model, \n",
        "                  model_state_file=args.model_state_file, \n",
        "                  save_dir=args.save_dir, device=args.device,\n",
        "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
        "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
        "                  early_stopping_criteria=args.early_stopping_criteria)\n",
        "trainer.run_train_loop()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[EPOCH]: 00 | [LR]: 0.001 | [TRAIN LOSS]: 0.77 | [TRAIN ACC]: 69.6% | [VAL LOSS]: 0.55 | [VAL ACC]: 80.4%\n",
            "[EPOCH]: 01 | [LR]: 0.001 | [TRAIN LOSS]: 0.50 | [TRAIN ACC]: 82.0% | [VAL LOSS]: 0.49 | [VAL ACC]: 82.2%\n",
            "[EPOCH]: 02 | [LR]: 0.001 | [TRAIN LOSS]: 0.44 | [TRAIN ACC]: 84.4% | [VAL LOSS]: 0.45 | [VAL ACC]: 83.9%\n",
            "[EPOCH]: 03 | [LR]: 0.001 | [TRAIN LOSS]: 0.39 | [TRAIN ACC]: 86.1% | [VAL LOSS]: 0.45 | [VAL ACC]: 83.9%\n",
            "[EPOCH]: 04 | [LR]: 0.001 | [TRAIN LOSS]: 0.36 | [TRAIN ACC]: 87.3% | [VAL LOSS]: 0.44 | [VAL ACC]: 84.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzRJIz88Flfe",
        "colab_type": "code",
        "outputId": "152606d5-ce05-4c19-8cca-4493a5c4c6e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "# Plot performance\n",
        "trainer.plot_performance()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAAE/CAYAAADVKysfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npqJNRERERETEw1S0iYiIiIiIeNj/AosRSKYlU241AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EmFhiX-FMaV",
        "colab_type": "code",
        "outputId": "b84052fb-0ad5-476d-89cb-43a6def847e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Test performance\n",
        "trainer.run_test_loop()\n",
        "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
        "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.43\n",
            "Test Accuracy: 84.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVU1zakYFMVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save all results\n",
        "trainer.save_train_state()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLoKfjSpFw7t",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51_PWM3xgXHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Inference(object):\n",
        "    def __init__(self, model, vectorizer, device=\"cpu\"):\n",
        "        self.model = model.to(device)\n",
        "        self.vectorizer = vectorizer\n",
        "        self.device = device\n",
        "  \n",
        "    def predict_category(self, dataset):\n",
        "        # Batch generator\n",
        "        batch_generator = dataset.generate_batches(\n",
        "            batch_size=len(dataset), shuffle=False, device=self.device)\n",
        "        self.model.eval()\n",
        "        \n",
        "        # Predict\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # compute the output\n",
        "            attn_scores, y_pred =  self.model(\n",
        "                x_word=batch_dict['title_word_vector'],\n",
        "                x_char=batch_dict['title_char_vector'],\n",
        "                x_lengths=batch_dict['title_length'],\n",
        "                device=self.device,\n",
        "                apply_softmax=True)\n",
        "\n",
        "            # Top k nationalities\n",
        "            y_prob, indices = torch.topk(y_pred, k=len(self.vectorizer.category_vocab))\n",
        "            probabilities = y_prob.detach().to('cpu').numpy()[0]\n",
        "            indices = indices.detach().to('cpu').numpy()[0]\n",
        "\n",
        "            results = []\n",
        "            for probability, index in zip(probabilities, indices):\n",
        "                category = self.vectorizer.category_vocab.lookup_index(index)\n",
        "                results.append({'category': category, \n",
        "                                'probability': probability})\n",
        "\n",
        "        return attn_scores, results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8UoSJPggXC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load vectorizer\n",
        "with open(args.vectorizer_file) as fp:\n",
        "    vectorizer = NewsVectorizer.from_serializable(json.load(fp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woxuiUA2gXAX",
        "colab_type": "code",
        "outputId": "b9dfcda7-6a78-4871-99a6-a418dd0bbd1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# Load the model\n",
        "model = NewsModel(embedding_dim=args.embedding_dim, \n",
        "                  num_word_embeddings=len(vectorizer.title_word_vocab), \n",
        "                  num_char_embeddings=len(vectorizer.title_char_vocab),\n",
        "                  kernels=args.kernels,\n",
        "                  num_input_channels=args.embedding_dim,\n",
        "                  num_output_channels=args.num_filters,\n",
        "                  rnn_hidden_dim=args.rnn_hidden_dim,\n",
        "                  hidden_dim=args.hidden_dim,\n",
        "                  output_dim=len(vectorizer.category_vocab),\n",
        "                  num_layers=args.num_layers,\n",
        "                  bidirectional=args.bidirectional,\n",
        "                  dropout_p=args.dropout_p, \n",
        "                  word_padding_idx=vectorizer.title_word_vocab.mask_index,\n",
        "                  char_padding_idx=vectorizer.title_char_vocab.mask_index)\n",
        "model.load_state_dict(torch.load(args.model_state_file))\n",
        "print (model.named_modules)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.named_modules of NewsModel(\n",
            "  (encoder): NewsEncoder(\n",
            "    (word_embeddings): Embedding(3406, 100, padding_idx=0)\n",
            "    (char_embeddings): Embedding(35, 100, padding_idx=0)\n",
            "    (conv): ModuleList(\n",
            "      (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "      (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
            "    )\n",
            "    (gru): GRU(300, 128, batch_first=True)\n",
            "  )\n",
            "  (decoder): NewsDecoder(\n",
            "    (fc_attn): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (dropout): Dropout(p=0.25)\n",
            "    (fc1): Linear(in_features=128, out_features=200, bias=True)\n",
            "    (fc2): Linear(in_features=200, out_features=4, bias=True)\n",
            "  )\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yfc666nngW9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize\n",
        "inference = Inference(model=model, vectorizer=vectorizer, device=\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1soPBCApgW7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InferenceDataset(Dataset):\n",
        "    def __init__(self, df, vectorizer):\n",
        "        self.df = df\n",
        "        self.vectorizer = vectorizer\n",
        "        self.target_size = len(self.df)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Dataset(size={1})>\".format(self.target_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        title_word_vector, title_char_vector, title_length = \\\n",
        "            self.vectorizer.vectorize(row.title)\n",
        "        return {'title_word_vector': title_word_vector, \n",
        "                'title_char_vector': title_char_vector, \n",
        "                'title_length': title_length}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        return len(self) // batch_size\n",
        "\n",
        "    def generate_batches(self, batch_size, shuffle=True, drop_last=False, device=\"cpu\"):\n",
        "        dataloader = DataLoader(dataset=self, batch_size=batch_size, \n",
        "                                shuffle=shuffle, drop_last=drop_last)\n",
        "        for data_dict in dataloader:\n",
        "            out_data_dict = {}\n",
        "            for name, tensor in data_dict.items():\n",
        "                out_data_dict[name] = data_dict[name].to(device)\n",
        "            yield out_data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU70BuYvgW4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inference\n",
        "title = input(\"Enter a title to classify: \")\n",
        "infer_df = pd.DataFrame([title], columns=['title'])\n",
        "infer_df.title = infer_df.title.apply(preprocess_text)\n",
        "infer_dataset = InferenceDataset(infer_df, vectorizer)\n",
        "attn_scores, results = inference.predict_category(dataset=infer_dataset)\n",
        "results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3jrZ6ZkxN4r",
        "colab_type": "text"
      },
      "source": [
        "# Interpretability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrAieHoHxOt2",
        "colab_type": "text"
      },
      "source": [
        "We can inspect the probability vector that is generated at each time step to visualize the importance of each of the previous hidden states towards a particular time step's prediction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6uZY4J8vYgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQwfUfwzjE_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attn_matrix = attn_scores.detach().numpy()\n",
        "ax = sns.heatmap(attn_matrix, linewidths=2, square=True)\n",
        "tokens = [\"<BEGIN>\"]+preprocess_text(title).split(\" \")+[\"<END>\"]\n",
        "ax.set_xticklabels(tokens, rotation=45)\n",
        "ax.set_xlabel(\"Token\")\n",
        "ax.set_ylabel(\"Importance\\n\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CWwxX2aTBla",
        "colab_type": "text"
      },
      "source": [
        "# Layer normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPzmk6T3TD_9",
        "colab_type": "text"
      },
      "source": [
        "Recall from our [CNN notebook](https://colab.research.google.com/github/GokuMohandas/practicalAI/blob/master/notebooks/11_Convolutional_Neural_Networks.ipynb) that we used batch normalization to deal with internal covariant shift. Our activations will experience the same issues with RNNs but we will use a technique known as [layer normalization](https://arxiv.org/abs/1607.06450) (layernorm) to maintain zero mean unit variance on the activations. \n",
        "\n",
        "With layernorm it's a bit different from batchnorm. We compute the mean and var for every single sample (instead of each hidden dim) for each layer independentlyand then do theoperations on the activations before they go through the nonlinearities. PyTorch's [LayerNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm) class abstracts all of this for us when we feed in inputs to the layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZTdk6OyTGq5",
        "colab_type": "text"
      },
      "source": [
        "$ LN = \\frac{a - \\mu_{L}}{\\sqrt{\\sigma^2_{L} + \\epsilon}}  * \\gamma + \\beta $\n",
        "\n",
        "where:\n",
        "* $a$ = activation | $\\in \\mathbb{R}^{NXH}$ ($N$ is the number of samples, $H$ is the hidden dim)\n",
        "* $ \\mu_{L}$ = mean of input| $\\in \\mathbb{R}^{NX1}$\n",
        "* $\\sigma^2_{L}$ = variance of input | $\\in \\mathbb{R}^{NX1}$\n",
        "* $epsilon$ = noise\n",
        "* $\\gamma$ = scale parameter (learned parameter)\n",
        "* $\\beta$ = shift parameter (learned parameter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAZGISTXTGnV",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/layernorm.png\" width=400>\n",
        "\n",
        "The most useful location to apply layernorm will be inside the RNN on the activations before the non-linearities. However, this is a bit involved and though PyTorch has a [LayerNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm) class, they do not have an RNN that has built in layernorm yet. You could implement the RNN yourself and manually add layernorm by following a similar setup like below.\n",
        "\n",
        "```python\n",
        "# Layernorm\n",
        "for t in range(seq_size):\n",
        "    # Normalize over hidden dim\n",
        "    layernorm = nn.LayerNorm(args.hidden_dim)\n",
        "    # Activating the module\n",
        "    a = layernorm(x)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YHneO3SStOp",
        "colab_type": "text"
      },
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGHaKTe1SuEk",
        "colab_type": "text"
      },
      "source": [
        "- attn visualization isn't always great\n",
        "- bleu score\n",
        "- ngram-overlap\n",
        "- perplexity\n",
        "- beamsearch\n",
        "- hierarchical softmax\n",
        "- hierarchical attention\n",
        "- Transformer networks\n",
        "- attention interpretability is hit/miss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9vzXj3I7azs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}